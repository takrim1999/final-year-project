{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## EfficientNet vs CBAM Attention: ODIR-5K Comparison\n",
        "\n",
        "This notebook compares a plain EfficientNet classifier to an EfficientNet+CBAM attention variant on ODIR-5K.\n",
        "\n",
        "- Baseline: EfficientNetB0/B3 (no attention)\n",
        "- Variant: EfficientNet + CBAM block on feature maps\n",
        "- Same data splits, preprocessing, and hyperparameters\n",
        "- Outputs: metrics table (accuracy, weighted/macro F1, ROC-AUC, PR-AUC), confusion matrices, and training curves\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, re, cv2, numpy as np, pandas as pd, matplotlib.pyplot as plt, seaborn as sns\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.applications import EfficientNetB0, EfficientNetB3\n",
        "from tensorflow.keras.applications.efficientnet import preprocess_input as effnet_preprocess\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, average_precision_score\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "# Config\n",
        "DATA_DIR = \"/kaggle/input/ocular-disease-recognition-odir5k\"\n",
        "OUTPUT_DIR = \"/kaggle/working\"\n",
        "IMAGE_SIZE = 224\n",
        "BACKBONE = \"b0\"\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 40\n",
        "SEED = 42\n",
        "USE_TFA = False\n",
        "try:\n",
        "    import tensorflow_addons as tfa\n",
        "    USE_TFA = True\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# Seed + mixed precision\n",
        "tf.keras.utils.set_random_seed(SEED)\n",
        "try:\n",
        "    from tensorflow.keras import mixed_precision\n",
        "    mixed_precision.set_global_policy('mixed_float16')\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# Augmentation\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "    layers.RandomFlip(\"horizontal\"),\n",
        "    layers.RandomRotation(0.05),\n",
        "    layers.RandomZoom(0.1),\n",
        "    layers.RandomContrast(0.1),\n",
        "], name=\"augment\")\n",
        "\n",
        "# CBAM\n",
        "@tf.keras.utils.register_keras_serializable(package=\"custom\")\n",
        "class CBAM(layers.Layer):\n",
        "    def __init__(self, reduction_ratio: int = 16, kernel_size: int = 7, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.reduction_ratio = reduction_ratio\n",
        "        self.kernel_size = kernel_size\n",
        "    def build(self, input_shape):\n",
        "        channels = int(input_shape[-1])\n",
        "        hidden = max(channels // self.reduction_ratio, 1)\n",
        "        self.mlp = tf.keras.Sequential([\n",
        "            layers.Dense(hidden, activation=\"relu\"),\n",
        "            layers.Dense(channels)\n",
        "        ])\n",
        "        self.spatial_conv = layers.Conv2D(1, kernel_size=self.kernel_size, padding=\"same\", activation=\"sigmoid\")\n",
        "        super().build(input_shape)\n",
        "    def call(self, x):\n",
        "        avg_pool = tf.reduce_mean(x, axis=[1,2], keepdims=True)\n",
        "        max_pool = tf.reduce_max(x, axis=[1,2], keepdims=True)\n",
        "        mlp_avg = self.mlp(layers.Flatten()(avg_pool))\n",
        "        mlp_max = self.mlp(layers.Flatten()(max_pool))\n",
        "        channel_attn = tf.nn.sigmoid(mlp_avg + mlp_max)\n",
        "        channel_attn = tf.reshape(channel_attn, (-1,1,1,tf.shape(x)[-1]))\n",
        "        x = x * channel_attn\n",
        "        avg_pool_sp = tf.reduce_mean(x, axis=-1, keepdims=True)\n",
        "        max_pool_sp = tf.reduce_max(x, axis=-1, keepdims=True)\n",
        "        sp = tf.concat([avg_pool_sp, max_pool_sp], axis=-1)\n",
        "        spatial_attn = self.spatial_conv(sp)\n",
        "        x = x * spatial_attn\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ensure output directory exists\n",
        "import os\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Parse ODIR-5K (single-label 5-class: G, C, A, H, M)\n",
        "ODIR_DIR = os.path.join(DATA_DIR, \"ODIR-5K\", \"ODIR-5K\")\n",
        "EXCEL_PATH = os.path.join(ODIR_DIR, \"data.xlsx\")\n",
        "TRAIN_IMG_DIR = os.path.join(ODIR_DIR, \"Training Images\")\n",
        "TEST_IMG_DIR = os.path.join(ODIR_DIR, \"Testing Images\")\n",
        "\n",
        "meta = pd.read_excel(EXCEL_PATH)\n",
        "\n",
        "def find_col(df, substrings):\n",
        "    subs = [s.lower() for s in substrings]\n",
        "    for c in df.columns:\n",
        "        lc = str(c).lower()\n",
        "        if all(s in lc for s in subs):\n",
        "            return c\n",
        "    return None\n",
        "\n",
        "left_img_col = find_col(meta, [\"left\",\"fundus\"]) or find_col(meta, [\"left\",\"image\"]) \n",
        "right_img_col = find_col(meta, [\"right\",\"fundus\"]) or find_col(meta, [\"right\",\"image\"]) \n",
        "left_diag_col = find_col(meta, [\"left\",\"diagn\"]) or find_col(meta, [\"left\",\"keyword\"]) \n",
        "right_diag_col = find_col(meta, [\"right\",\"diagn\"]) or find_col(meta, [\"right\",\"keyword\"]) \n",
        "\n",
        "KEYWORD_TO_SHORT = {\n",
        "    \"glaucoma\":\"G\",\"cataract\":\"C\",\"amd\":\"A\",\"age-related macular degeneration\":\"A\",\"age related macular degeneration\":\"A\",\n",
        "    \"hypertension\":\"H\",\"hypertensive\":\"H\",\"hypertensive retinopathy\":\"H\",\"htn\":\"H\",\n",
        "    \"myopia\":\"M\",\"normal\":\"N\",\"diabetic retinopathy\":\"D\",\"dr\":\"D\",\"other\":\"O\",\"others\":\"O\"\n",
        "}\n",
        "\n",
        "TARGET = [\"G\",\"C\",\"A\",\"H\",\"M\"]\n",
        "\n",
        "records = []\n",
        "for _, row in meta.iterrows():\n",
        "    for img_col, diag_col in [(left_img_col,left_diag_col),(right_img_col,right_diag_col)]:\n",
        "        fname = row.get(img_col)\n",
        "        if not isinstance(fname,str) or not fname:\n",
        "            continue\n",
        "        text = row.get(diag_col) if diag_col in meta.columns else None\n",
        "        text_l = str(text).lower() if isinstance(text,str) else \"\"\n",
        "        labels = set()\n",
        "        for k,s in KEYWORD_TO_SHORT.items():\n",
        "            if k in text_l:\n",
        "                labels.add(s)\n",
        "        labels = [l for l in labels if l in TARGET]\n",
        "        if not labels:\n",
        "            continue\n",
        "        # single-label preference: if H present, keep H; else first in TARGET order\n",
        "        final = \"H\" if \"H\" in labels else [l for l in TARGET if l in labels][0]\n",
        "        records.append({\"filename\": fname, \"label\": final})\n",
        "\n",
        "df = pd.DataFrame.from_records(records)\n",
        "\n",
        "# Resolve paths\n",
        "df[\"path\"] = [os.path.join(TRAIN_IMG_DIR, f) if os.path.exists(os.path.join(TRAIN_IMG_DIR, f)) else os.path.join(TEST_IMG_DIR, f) for f in df[\"filename\"].values]\n",
        "df = df[df[\"path\"].apply(os.path.exists)].reset_index(drop=True)\n",
        "print(\"Counts by label:\\n\", df[\"label\"].value_counts())\n",
        "\n",
        "# Robust stratified split\n",
        "sss1 = StratifiedShuffleSplit(n_splits=200, test_size=0.3, random_state=SEED)\n",
        "labels = df[\"label\"].values\n",
        "chosen = None\n",
        "for tr_idx, tmp_idx in sss1.split(df, labels):\n",
        "    tr = df.iloc[tr_idx]\n",
        "    tmp = df.iloc[tmp_idx]\n",
        "    sss2 = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=SEED)\n",
        "    for va_idx, te_idx in sss2.split(tmp, tmp[\"label\" ].values):\n",
        "        va = tmp.iloc[va_idx]\n",
        "        te = tmp.iloc[te_idx]\n",
        "        if set(va[\"label\"].unique()) == set(TARGET) and set(te[\"label\"].unique()) == set(TARGET):\n",
        "            chosen = (tr.reset_index(drop=True), va.reset_index(drop=True), te.reset_index(drop=True))\n",
        "            break\n",
        "    if chosen:\n",
        "        break\n",
        "if not chosen:\n",
        "    print(\"Warning: fallback split used.\")\n",
        "    chosen = (tr.reset_index(drop=True), va.reset_index(drop=True), te.reset_index(drop=True))\n",
        "train_df, val_df, test_df = chosen\n",
        "print(\"Split sizes:\", len(train_df), len(val_df), len(test_df))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Arrays\n",
        "\n",
        "def load_and_preprocess(path):\n",
        "    img = cv2.imread(path)\n",
        "    if img is None:\n",
        "        img = np.zeros((IMAGE_SIZE, IMAGE_SIZE, 3), dtype=np.uint8)\n",
        "    else:\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        img = cv2.resize(img, (IMAGE_SIZE, IMAGE_SIZE))\n",
        "    return img.astype(\"float32\")\n",
        "\n",
        "class_to_idx = {c:i for i,c in enumerate([\"G\",\"C\",\"A\",\"H\",\"M\"])}\n",
        "num_classes = 5\n",
        "\n",
        "def df_to_arrays(df):\n",
        "    xs = np.stack([load_and_preprocess(p) for p in df[\"path\"].values], axis=0)\n",
        "    ys = np.array([class_to_idx[c] for c in df[\"label\"].values])\n",
        "    ys = tf.keras.utils.to_categorical(ys, num_classes=num_classes)\n",
        "    return xs, ys\n",
        "\n",
        "x_train, y_train = df_to_arrays(train_df)\n",
        "x_val, y_val = df_to_arrays(val_df)\n",
        "x_test, y_test = df_to_arrays(test_df)\n",
        "print(x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Class weights to rebalance Hypertension\n",
        "from collections import Counter\n",
        "APPLY_CLASS_WEIGHTS = True\n",
        "\n",
        "train_labels = [class_to_idx[c] for c in train_df[\"label\"].values]\n",
        "cnt = Counter(train_labels)\n",
        "class_weight = {i: (len(train_labels) / (num_classes * cnt.get(i, 1))) for i in range(num_classes)}\n",
        "print(\"Class weights:\", class_weight)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model builders\n",
        "\n",
        "def build_effnet_baseline(image_size=224, backbone=\"b0\", num_classes=5, dropout=0.4):\n",
        "    inputs = layers.Input(shape=(image_size, image_size, 3))\n",
        "    x_in = data_augmentation(inputs)\n",
        "    x_in = effnet_preprocess(x_in)\n",
        "    if backbone == \"b3\":\n",
        "        base = EfficientNetB3(include_top=False, weights=\"imagenet\", input_tensor=x_in)\n",
        "    else:\n",
        "        base = EfficientNetB0(include_top=False, weights=\"imagenet\", input_tensor=x_in)\n",
        "    for l in base.layers:\n",
        "        l.trainable = True\n",
        "    x = base.output\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Conv2D(192, 1, activation=\"relu\")(x)\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    x = layers.Dense(192, activation=\"relu\")(x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    outputs = layers.Dense(num_classes, activation=\"softmax\", dtype=\"float32\")(x)\n",
        "    return models.Model(inputs, outputs)\n",
        "\n",
        "@tf.keras.utils.register_keras_serializable(package=\"custom\")\n",
        "class CBAM(layers.Layer):\n",
        "    def __init__(self, reduction_ratio: int = 16, kernel_size: int = 7, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.reduction_ratio = reduction_ratio\n",
        "        self.kernel_size = kernel_size\n",
        "    def build(self, input_shape):\n",
        "        channels = int(input_shape[-1])\n",
        "        hidden = max(channels // self.reduction_ratio, 1)\n",
        "        self.mlp = tf.keras.Sequential([\n",
        "            layers.Dense(hidden, activation=\"relu\"),\n",
        "            layers.Dense(channels)\n",
        "        ])\n",
        "        self.spatial_conv = layers.Conv2D(1, kernel_size=self.kernel_size, padding=\"same\", activation=\"sigmoid\")\n",
        "        super().build(input_shape)\n",
        "    def call(self, x):\n",
        "        avg_pool = tf.reduce_mean(x, axis=[1,2], keepdims=True)\n",
        "        max_pool = tf.reduce_max(x, axis=[1,2], keepdims=True)\n",
        "        mlp_avg = self.mlp(layers.Flatten()(avg_pool))\n",
        "        mlp_max = self.mlp(layers.Flatten()(max_pool))\n",
        "        channel_attn = tf.nn.sigmoid(mlp_avg + mlp_max)\n",
        "        channel_attn = tf.reshape(channel_attn, (-1,1,1,tf.shape(x)[-1]))\n",
        "        x = x * channel_attn\n",
        "        avg_pool_sp = tf.reduce_mean(x, axis=-1, keepdims=True)\n",
        "        max_pool_sp = tf.reduce_max(x, axis=-1, keepdims=True)\n",
        "        sp = tf.concat([avg_pool_sp, max_pool_sp], axis=-1)\n",
        "        spatial_attn = self.spatial_conv(sp)\n",
        "        return x * spatial_attn\n",
        "\n",
        "def build_effnet_cbam(image_size=224, backbone=\"b0\", num_classes=5, dropout=0.4):\n",
        "    inputs = layers.Input(shape=(image_size, image_size, 3))\n",
        "    x_in = data_augmentation(inputs)\n",
        "    x_in = effnet_preprocess(x_in)\n",
        "    if backbone == \"b3\":\n",
        "        base = EfficientNetB3(include_top=False, weights=\"imagenet\", input_tensor=x_in)\n",
        "    else:\n",
        "        base = EfficientNetB0(include_top=False, weights=\"imagenet\", input_tensor=x_in)\n",
        "    for l in base.layers:\n",
        "        l.trainable = True\n",
        "    x = base.output\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = CBAM()(x)\n",
        "    x = layers.Conv2D(192, 1, activation=\"relu\")(x)\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    x = layers.Dense(192, activation=\"relu\")(x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    outputs = layers.Dense(num_classes, activation=\"softmax\", dtype=\"float32\")(x)\n",
        "    return models.Model(inputs, outputs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train/eval utilities\n",
        "\n",
        "def train_and_eval(model, name, x_train, y_train, x_val, y_val, x_test, y_test, lr=3e-4):\n",
        "    METRICS = [\n",
        "        tf.keras.metrics.CategoricalAccuracy(name=\"acc\"),\n",
        "        tf.keras.metrics.AUC(name=\"auc\"),\n",
        "        tf.keras.metrics.AUC(name=\"prc\", curve=\"PR\"),\n",
        "    ]\n",
        "    if USE_TFA:\n",
        "        METRICS.append(tfa.metrics.F1Score(num_classes=y_train.shape[1], average=\"weighted\", name=\"f1\"))\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr), loss=\"categorical_crossentropy\", metrics=METRICS)\n",
        "    ckpt_path = os.path.join(OUTPUT_DIR, f\"best_{name}.keras\")\n",
        "    callbacks = [\n",
        "        ModelCheckpoint(ckpt_path, save_best_only=True, monitor=\"val_acc\", mode=\"max\"),\n",
        "        ReduceLROnPlateau(factor=0.5, patience=5, min_lr=1e-6, verbose=1),\n",
        "        EarlyStopping(patience=10, restore_best_weights=True, monitor=\"val_acc\", mode=\"max\", verbose=1),\n",
        "    ]\n",
        "    _ = model.predict(x_train[:4], verbose=0)\n",
        "    hist = model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=EPOCHS, batch_size=BATCH_SIZE, callbacks=callbacks, verbose=1)\n",
        "\n",
        "    # Save training curves\n",
        "    h = hist.history\n",
        "    def plot_curve(tr, va, title):\n",
        "        if tr in h and va in h:\n",
        "            plt.figure()\n",
        "            plt.plot(h[tr], label=tr)\n",
        "            plt.plot(h[va], label=va)\n",
        "            plt.title(f'{name}: {title}')\n",
        "            plt.xlabel('Epochs')\n",
        "            plt.legend()\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(os.path.join(OUTPUT_DIR, f'{name}_{tr}.png'))\n",
        "            plt.show()\n",
        "            plt.close()\n",
        "    plot_curve('acc','val_acc','Accuracy')\n",
        "    plot_curve('loss','val_loss','Loss')\n",
        "    plot_curve('auc','val_auc','ROC-AUC')\n",
        "    plot_curve('prc','val_prc','PR-AUC')\n",
        "\n",
        "    # Evaluate\n",
        "    y_prob = model.predict(x_test, batch_size=BATCH_SIZE, verbose=0)\n",
        "    y_pred = np.argmax(y_prob, axis=1)\n",
        "    y_true = np.argmax(y_test, axis=1)\n",
        "    labels = list(range(y_test.shape[1]))\n",
        "    names_full = [\"Glaucoma\",\"Cataract\",\"AMD\",\"Hypertension\",\"Myopia\"]\n",
        "\n",
        "    # Confusion matrix (counts)\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
        "    plt.figure(figsize=(7,6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=names_full, yticklabels=names_full)\n",
        "    plt.title(f'Confusion Matrix (Test) - {name}')\n",
        "    plt.xlabel('Predicted label'); plt.ylabel('True label')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(OUTPUT_DIR, f'cm_{name}_counts.png'))\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "    # Save confusion matrix as CSV\n",
        "    pd.DataFrame(cm, index=names_full, columns=names_full).to_csv(os.path.join(OUTPUT_DIR, f'cm_{name}.csv'))\n",
        "\n",
        "    # Classification metrics\n",
        "    report = classification_report(y_true, y_pred, labels=labels, target_names=names_full, zero_division=0, output_dict=True)\n",
        "    with open(os.path.join(OUTPUT_DIR, f'classification_report_{name}.txt'),'w') as f:\n",
        "        for k,v in report.items():\n",
        "            f.write(f'{k}: {v}\\n')\n",
        "\n",
        "    # ROC/PR curves per class\n",
        "    from sklearn.metrics import roc_curve, auc, precision_recall_curve\n",
        "    present = sorted(list(set(y_true)))\n",
        "    y_true_bin = tf.keras.utils.to_categorical(y_true, num_classes=y_test.shape[1])\n",
        "\n",
        "    plt.figure(figsize=(7,6))\n",
        "    for c in present:\n",
        "        fpr, tpr, _ = roc_curve(y_true_bin[:,c], y_prob[:,c])\n",
        "        plt.plot(fpr, tpr, label=names_full[c])\n",
        "    plt.plot([0,1],[0,1],'k--',alpha=0.5)\n",
        "    plt.title(f'ROC Curves - {name}')\n",
        "    plt.xlabel('FPR'); plt.ylabel('TPR'); plt.legend(); plt.tight_layout()\n",
        "    plt.savefig(os.path.join(OUTPUT_DIR, f'roc_curves_{name}.png'))\n",
        "    plt.close()\n",
        "\n",
        "    plt.figure(figsize=(7,6))\n",
        "    for c in present:\n",
        "        precision, recall, _ = precision_recall_curve(y_true_bin[:,c], y_prob[:,c])\n",
        "        plt.plot(recall, precision, label=names_full[c])\n",
        "    plt.title(f'PR Curves - {name}')\n",
        "    plt.xlabel('Recall'); plt.ylabel('Precision'); plt.legend(); plt.tight_layout()\n",
        "    plt.savefig(os.path.join(OUTPUT_DIR, f'pr_curves_{name}.png'))\n",
        "    plt.close()\n",
        "\n",
        "    # Macro AUCs\n",
        "    roc_auc = roc_auc_score(y_true_bin[:,present], y_prob[:,present], average='macro', multi_class='ovr')\n",
        "    pr_auc = average_precision_score(y_true_bin[:,present], y_prob[:,present], average='macro')\n",
        "    res = {\n",
        "        \"name\": name,\n",
        "        \"acc\": report[\"accuracy\"],\n",
        "        \"macro_f1\": report[\"macro avg\"][\"f1-score\"],\n",
        "        \"weighted_f1\": report[\"weighted avg\"][\"f1-score\"],\n",
        "        \"roc_auc_macro\": float(roc_auc),\n",
        "        \"pr_auc_macro\": float(pr_auc),\n",
        "    }\n",
        "    return res, h\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run comparison\n",
        "\n",
        "# Baseline\n",
        "baseline = build_effnet_baseline(image_size=IMAGE_SIZE, backbone=BACKBONE, num_classes=num_classes, dropout=0.4)\n",
        "res_base, hist_base = train_and_eval(baseline, \"effnet_baseline\", x_train, y_train, x_val, y_val, x_test, y_test)\n",
        "\n",
        "# CBAM\n",
        "cbam = build_effnet_cbam(image_size=IMAGE_SIZE, backbone=BACKBONE, num_classes=num_classes, dropout=0.4)\n",
        "res_cbam, hist_cbam = train_and_eval(cbam, \"effnet_cbam\", x_train, y_train, x_val, y_val, x_test, y_test)\n",
        "\n",
        "# Compare\n",
        "import pandas as pd\n",
        "comp = pd.DataFrame([res_base, res_cbam])\n",
        "print(comp)\n",
        "comp.to_csv(os.path.join(OUTPUT_DIR, \"effnet_vs_cbam_metrics.csv\"), index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Baseline EfficientNet model (no attention)\n",
        "\n",
        "def build_effnet_baseline(image_size=224, backbone=\"b0\", num_classes=5, dropout=0.4):\n",
        "    inputs = layers.Input(shape=(image_size, image_size, 3))\n",
        "    x_in = data_augmentation(inputs)\n",
        "    x_in = effnet_preprocess(x_in)\n",
        "    if backbone == \"b3\":\n",
        "        base = EfficientNetB3(include_top=False, weights=\"imagenet\", input_tensor=x_in)\n",
        "    else:\n",
        "        base = EfficientNetB0(include_top=False, weights=\"imagenet\", input_tensor=x_in)\n",
        "    for l in base.layers:\n",
        "        l.trainable = True\n",
        "    x = base.output\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Conv2D(192, 1, activation=\"relu\")(x)\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    x = layers.Dense(192, activation=\"relu\")(x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    outputs = layers.Dense(num_classes, activation=\"softmax\", dtype=\"float32\")(x)\n",
        "    return models.Model(inputs, outputs)\n",
        "\n",
        "# CBAM variant\n",
        "def build_effnet_cbam(image_size=224, backbone=\"b0\", num_classes=5, dropout=0.4):\n",
        "    inputs = layers.Input(shape=(image_size, image_size, 3))\n",
        "    x_in = data_augmentation(inputs)\n",
        "    x_in = effnet_preprocess(x_in)\n",
        "    if backbone == \"b3\":\n",
        "        base = EfficientNetB3(include_top=False, weights=\"imagenet\", input_tensor=x_in)\n",
        "    else:\n",
        "        base = EfficientNetB0(include_top=False, weights=\"imagenet\", input_tensor=x_in)\n",
        "    for l in base.layers:\n",
        "        l.trainable = True\n",
        "    x = base.output\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = CBAM()(x)\n",
        "    x = layers.Conv2D(192, 1, activation=\"relu\")(x)\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    x = layers.Dense(192, activation=\"relu\")(x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    outputs = layers.Dense(num_classes, activation=\"softmax\", dtype=\"float32\")(x)\n",
        "    return models.Model(inputs, outputs)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
