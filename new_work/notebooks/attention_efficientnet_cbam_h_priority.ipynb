{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Attention-based ODIR Classifier (Hypertension-Priority)\n",
        "\n",
        "This notebook prioritizes Hypertension labeling: if a sample contains Hypertension among multiple diagnoses, it is labeled as Hypertension (H). Other classes remain single-label: G, C, A, M.\n",
        "\n",
        "- Backbone: EfficientNetB0 (optionally B3)\n",
        "- Attention: CBAM on CNN feature maps\n",
        "- Split: robust stratified to ensure all 5 classes appear in val/test\n",
        "- Metrics: accuracy, weighted/macro F1, ROC-AUC (OvR), PR-AUC\n",
        "- Outputs saved to `/kaggle/working/`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.applications import EfficientNetB0, EfficientNetB3\n",
        "from tensorflow.keras.applications.efficientnet import preprocess_input as effnet_preprocess\n",
        "\n",
        "# Config\n",
        "DATA_DIR = \"/kaggle/input/ocular-disease-recognition-odir5k\"\n",
        "OUTPUT_DIR = \"/kaggle/working\"\n",
        "IMAGE_SIZE = 224\n",
        "BACKBONE = \"b0\"\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 40\n",
        "SEED = 42\n",
        "\n",
        "USE_TFA = False\n",
        "try:\n",
        "    import tensorflow_addons as tfa\n",
        "    USE_TFA = True\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# Seed + mixed precision\n",
        "tf.keras.utils.set_random_seed(SEED)\n",
        "try:\n",
        "    from tensorflow.keras import mixed_precision\n",
        "    mixed_precision.set_global_policy('mixed_float16')\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# Augmentation\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "    layers.RandomFlip(\"horizontal\"),\n",
        "    layers.RandomRotation(0.05),\n",
        "    layers.RandomZoom(0.1),\n",
        "    layers.RandomContrast(0.1),\n",
        "], name=\"augment\")\n",
        "\n",
        "# CBAM\n",
        "@tf.keras.utils.register_keras_serializable(package=\"custom\")\n",
        "class CBAM(layers.Layer):\n",
        "    def __init__(self, reduction_ratio: int = 16, kernel_size: int = 7, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.reduction_ratio = reduction_ratio\n",
        "        self.kernel_size = kernel_size\n",
        "    def build(self, input_shape):\n",
        "        channels = int(input_shape[-1])\n",
        "        hidden = max(channels // self.reduction_ratio, 1)\n",
        "        self.mlp = tf.keras.Sequential([\n",
        "            layers.Dense(hidden, activation=\"relu\"),\n",
        "            layers.Dense(channels)\n",
        "        ])\n",
        "        self.spatial_conv = layers.Conv2D(1, kernel_size=self.kernel_size, padding=\"same\", activation=\"sigmoid\")\n",
        "        super().build(input_shape)\n",
        "    def call(self, x):\n",
        "        avg_pool = tf.reduce_mean(x, axis=[1,2], keepdims=True)\n",
        "        max_pool = tf.reduce_max(x, axis=[1,2], keepdims=True)\n",
        "        mlp_avg = self.mlp(layers.Flatten()(avg_pool))\n",
        "        mlp_max = self.mlp(layers.Flatten()(max_pool))\n",
        "        channel_attn = tf.nn.sigmoid(mlp_avg + mlp_max)\n",
        "        channel_attn = tf.reshape(channel_attn, (-1,1,1,tf.shape(x)[-1]))\n",
        "        x = x * channel_attn\n",
        "        avg_pool_sp = tf.reduce_mean(x, axis=-1, keepdims=True)\n",
        "        max_pool_sp = tf.reduce_max(x, axis=-1, keepdims=True)\n",
        "        sp = tf.concat([avg_pool_sp, max_pool_sp], axis=-1)\n",
        "        spatial_attn = self.spatial_conv(sp)\n",
        "        x = x * spatial_attn\n",
        "        return x\n",
        "\n",
        "# Backbone\n",
        "def build_backbone(x_in, backbone=\"b0\", image_size=224):\n",
        "    if backbone == \"b3\":\n",
        "        base = EfficientNetB3(include_top=False, weights=\"imagenet\", input_tensor=x_in)\n",
        "    else:\n",
        "        base = EfficientNetB0(include_top=False, weights=\"imagenet\", input_tensor=x_in)\n",
        "    for l in base.layers:\n",
        "        l.trainable = True\n",
        "    return base\n",
        "\n",
        "# Model\n",
        "def build_model_cbam(image_size=224, backbone=\"b0\", num_classes=5, dropout=0.4):\n",
        "    tf.keras.backend.clear_session()\n",
        "    inputs = layers.Input(shape=(image_size, image_size, 3))\n",
        "    x_in = data_augmentation(inputs)\n",
        "    x_in = effnet_preprocess(x_in)\n",
        "    bb = build_backbone(x_in, backbone=backbone, image_size=image_size)\n",
        "    x = bb.output\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = CBAM()(x)\n",
        "    x = layers.Conv2D(192, 1, activation=\"relu\")(x)\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    x = layers.Dense(192, activation=\"relu\")(x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    outputs = layers.Dense(num_classes, activation=\"softmax\", dtype=\"float32\")(x)\n",
        "    return models.Model(inputs, outputs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Parse ODIR-5K with Hypertension-priority labeling\n",
        "ODIR_DIR = os.path.join(DATA_DIR, \"ODIR-5K\", \"ODIR-5K\")\n",
        "EXCEL_PATH = os.path.join(ODIR_DIR, \"data.xlsx\")\n",
        "TRAIN_IMG_DIR = os.path.join(ODIR_DIR, \"Training Images\")\n",
        "TEST_IMG_DIR = os.path.join(ODIR_DIR, \"Testing Images\")\n",
        "\n",
        "meta = pd.read_excel(EXCEL_PATH)\n",
        "\n",
        "def find_col(df, substrings):\n",
        "    subs = [s.lower() for s in substrings]\n",
        "    for c in df.columns:\n",
        "        lc = str(c).lower()\n",
        "        if all(s in lc for s in subs):\n",
        "            return c\n",
        "    return None\n",
        "\n",
        "left_img_col = find_col(meta, [\"left\",\"fundus\"]) or find_col(meta, [\"left\",\"image\"])\n",
        "right_img_col = find_col(meta, [\"right\",\"fundus\"]) or find_col(meta, [\"right\",\"image\"])\n",
        "left_diag_col = find_col(meta, [\"left\",\"diagn\"]) or find_col(meta, [\"left\",\"keyword\"]) \n",
        "right_diag_col = find_col(meta, [\"right\",\"diagn\"]) or find_col(meta, [\"right\",\"keyword\"]) \n",
        "\n",
        "KEYWORD_TO_SHORT = {\n",
        "    \"glaucoma\":\"G\",\"cataract\":\"C\",\"amd\":\"A\",\"age-related macular degeneration\":\"A\",\"age related macular degeneration\":\"A\",\n",
        "    \"hypertension\":\"H\",\"hypertensive\":\"H\",\"hypertensive retinopathy\":\"H\",\"htn\":\"H\",\n",
        "    \"myopia\":\"M\",\"normal\":\"N\",\"diabetic retinopathy\":\"D\",\"dr\":\"D\",\"other\":\"O\",\"others\":\"O\"\n",
        "}\n",
        "\n",
        "TARGET = [\"G\",\"C\",\"A\",\"H\",\"M\"]\n",
        "\n",
        "records = []\n",
        "for _, row in meta.iterrows():\n",
        "    for img_col, diag_col in [(left_img_col,left_diag_col),(right_img_col,right_diag_col)]:\n",
        "        fname = row.get(img_col)\n",
        "        if not isinstance(fname,str) or not fname:\n",
        "            continue\n",
        "        text = row.get(diag_col) if diag_col in meta.columns else None\n",
        "        text_l = str(text).lower() if isinstance(text,str) else \"\"\n",
        "        labels = set()\n",
        "        for k,s in KEYWORD_TO_SHORT.items():\n",
        "            if k in text_l:\n",
        "                labels.add(s)\n",
        "        labels = [l for l in labels if l in TARGET]\n",
        "        if not labels:\n",
        "            continue\n",
        "        # Hypertension-priority: if H present among multi-label, set to H; otherwise for multiple labels, pick first in TARGET order\n",
        "        if \"H\" in labels:\n",
        "            final = \"H\"\n",
        "        else:\n",
        "            # keep single-label if one; else choose a deterministic label by TARGET order\n",
        "            labels_sorted = [l for l in TARGET if l in labels]\n",
        "            final = labels_sorted[0]\n",
        "        records.append({\"filename\": fname, \"label\": final})\n",
        "\n",
        "df = pd.DataFrame.from_records(records)\n",
        "\n",
        "# Resolve paths\n",
        "paths = []\n",
        "for fname in df[\"filename\"].values:\n",
        "    p = os.path.join(TRAIN_IMG_DIR, fname)\n",
        "    if not os.path.exists(p):\n",
        "        p = os.path.join(TEST_IMG_DIR, fname)\n",
        "    paths.append(p)\n",
        "df[\"path\"] = paths\n",
        "\n",
        "df = df[df[\"path\"].apply(os.path.exists)].reset_index(drop=True)\n",
        "print(\"Counts by label:\\n\", df[\"label\"].value_counts())\n",
        "\n",
        "# Robust stratified split\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "def stratified_split_with_min(df_in, label_col=\"label\", val_size=0.15, test_size=0.15, seed=SEED, attempts=100):\n",
        "    sss1 = StratifiedShuffleSplit(n_splits=attempts, test_size=val_size+test_size, random_state=seed)\n",
        "    labels = df_in[label_col].values\n",
        "    last = None\n",
        "    for train_idx, temp_idx in sss1.split(df_in, labels):\n",
        "        train_df_cand = df_in.iloc[train_idx]\n",
        "        temp_df_cand = df_in.iloc[temp_idx]\n",
        "        sss2 = StratifiedShuffleSplit(n_splits=1, test_size=test_size/(val_size+test_size), random_state=seed)\n",
        "        for val_idx, test_idx in sss2.split(temp_df_cand, temp_df_cand[label_col].values):\n",
        "            val_df_cand = temp_df_cand.iloc[val_idx]\n",
        "            test_df_cand = temp_df_cand.iloc[test_idx]\n",
        "            last = (train_df_cand, val_df_cand, test_df_cand)\n",
        "            if set(val_df_cand[label_col].unique()) == set(TARGET) and set(test_df_cand[label_col].unique()) == set(TARGET):\n",
        "                return train_df_cand.reset_index(drop=True), val_df_cand.reset_index(drop=True), test_df_cand.reset_index(drop=True)\n",
        "    print(\"Warning: could not guarantee all classes in val/test; using best-effort stratified split.\")\n",
        "    train_df_cand, val_df_cand, test_df_cand = last\n",
        "    return train_df_cand.reset_index(drop=True), val_df_cand.reset_index(drop=True), test_df_cand.reset_index(drop=True)\n",
        "\n",
        "train_df, val_df, test_df = stratified_split_with_min(df)\n",
        "print(\"Split sizes:\", len(train_df), len(val_df), len(test_df))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build arrays\n",
        "\n",
        "def load_and_preprocess(path):\n",
        "    img = cv2.imread(path)\n",
        "    if img is None:\n",
        "        img = np.zeros((IMAGE_SIZE, IMAGE_SIZE, 3), dtype=np.uint8)\n",
        "    else:\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        img = cv2.resize(img, (IMAGE_SIZE, IMAGE_SIZE))\n",
        "    return img.astype(\"float32\")\n",
        "\n",
        "class_to_idx = {c:i for i,c in enumerate([\"G\",\"C\",\"A\",\"H\",\"M\"])}\n",
        "num_classes = 5\n",
        "\n",
        "def df_to_arrays(df):\n",
        "    xs = np.stack([load_and_preprocess(p) for p in df[\"path\"].values], axis=0)\n",
        "    ys = np.array([class_to_idx[c] for c in df[\"label\"].values])\n",
        "    ys = tf.keras.utils.to_categorical(ys, num_classes=num_classes)\n",
        "    return xs, ys\n",
        "\n",
        "x_train, y_train = df_to_arrays(train_df)\n",
        "x_val, y_val = df_to_arrays(val_df)\n",
        "x_test, y_test = df_to_arrays(test_df)\n",
        "print(x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train\n",
        "model = build_model_cbam(image_size=IMAGE_SIZE, backbone=BACKBONE, num_classes=num_classes, dropout=0.4)\n",
        "METRICS = [\n",
        "    tf.keras.metrics.CategoricalAccuracy(name=\"acc\"),\n",
        "    tf.keras.metrics.AUC(name=\"auc\"),\n",
        "    tf.keras.metrics.AUC(name=\"prc\", curve=\"PR\"),\n",
        "]\n",
        "if USE_TFA:\n",
        "    METRICS.append(tfa.metrics.F1Score(num_classes=num_classes, average=\"weighted\", name=\"f1\"))\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=3e-4), loss=\"categorical_crossentropy\", metrics=METRICS)\n",
        "ckpt_path = os.path.join(OUTPUT_DIR, \"best_attention_efficientnet_hprio.keras\")\n",
        "callbacks = [\n",
        "    ModelCheckpoint(ckpt_path, save_best_only=True, monitor=\"val_acc\", mode=\"max\"),\n",
        "    ReduceLROnPlateau(factor=0.5, patience=5, min_lr=1e-6, verbose=1),\n",
        "    EarlyStopping(patience=10, restore_best_weights=True, monitor=\"val_acc\", mode=\"max\", verbose=1),\n",
        "]\n",
        "_ = model.predict(x_train[:4], verbose=0)\n",
        "history = model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=EPOCHS, batch_size=BATCH_SIZE, callbacks=callbacks, verbose=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, average_precision_score\n",
        "\n",
        "best_model = tf.keras.models.load_model(ckpt_path, compile=False)\n",
        "best_model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=METRICS)\n",
        "\n",
        "_ = best_model.evaluate(x_test, y_test, verbose=0)\n",
        "y_prob = best_model.predict(x_test, batch_size=BATCH_SIZE, verbose=0)\n",
        "y_pred = np.argmax(y_prob, axis=1)\n",
        "y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "labels = list(range(num_classes))\n",
        "class_names_full = [\"Glaucoma\",\"Cataract\",\"AMD\",\"Hypertension\",\"Myopia\"]\n",
        "cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
        "cm_norm = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
        "plt.figure(figsize=(7,6))\n",
        "sns.heatmap(cm_norm*100, annot=True, fmt='.2f', cmap='Blues', xticklabels=class_names_full, yticklabels=class_names_full)\n",
        "plt.title('Confusion Matrix (Test) - H Priority')\n",
        "plt.xlabel('Predicted label')\n",
        "plt.ylabel('True label')\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUTPUT_DIR, 'confusion_matrix_test_hprio.png'))\n",
        "plt.show()\n",
        "\n",
        "print(classification_report(y_true, y_pred, labels=labels, target_names=class_names_full, zero_division=0))\n",
        "\n",
        "present = sorted(list(set(y_true)))\n",
        "y_true_bin = tf.keras.utils.to_categorical(y_true, num_classes=num_classes)\n",
        "try:\n",
        "    roc_auc = roc_auc_score(y_true_bin[:,present], y_prob[:,present], average='macro', multi_class='ovr')\n",
        "    pr_auc = average_precision_score(y_true_bin[:,present], y_prob[:,present], average='macro')\n",
        "    print('ROC-AUC (macro, OvR):', round(roc_auc, 4))\n",
        "    print('PR-AUC (macro):', round(pr_auc, 4))\n",
        "except Exception as e:\n",
        "    print('AUC error:', e)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
